# parameters
nc: 2  # number of classes
depth_multiple: 0.33  # model depth multiple
width_multiple: 0.5  # layer channel multiple

# anchors
anchors:
  # Anchor for CrowdHuman dataset, 都是竖长的
  - [65,188, 120,269, 189,486]  # P5/32
  - [22,60, 35,42, 42,106]  # P4/16
  - [5,5, 10,11, 18,22]  # P3/8

  # Anchors for COCO dataset 
  # - [116,90, 156,198, 373,326]  # P5/32
  # - [30,61, 62,45, 59,119]  # P4/16
  # - [10,13, 16,30, 33,23]  # P3/8

  # - [39,107, 69,181, 128,301]  # P5/32
  # - [35,28,22,59, 63,53]  # P4/16
  # - [8,6, 18,13, 12,29]  # P3/8

stride: 
  [32, 16, 8]

backbone:
  # [from, number, module, args]
  [[-1, 1, Focus, [64, 3]],  # 0-P1/2
   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
   [-1, 3, C3, [128]],
   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8
   [-1, 9, C3, [256]],
   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16
   [-1, 9, C3, [512]],
   [-1, 1, Conv, [1024, 3, 2]], # 7-P5/32
   [-1, 1, SPP, [1024, [5, 9, 13]]],
  ]


head:
  [[-1, 3, C3, [1024, False]],  # 9
   [-1, 1, Conv, [512, 1, 1]],
  #  [-1, 1, DeConv, [512, 4, 2, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],

   [[-1, 6], 1, Concat, [1]],  # cat backbone P4
   [-1, 3, C3, [512, False]],  # 13

   [-1, 1, Conv, [256, 1, 1]],
  #  [-1, 1, DeConv, [256, 4, 2, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],

   [[-1, 4], 1, Concat, [1]],  # cat backbone P3
   [-1, 3, C3, [256, False]],
   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1]],  # 18 (P3/8-small)

   [-2, 1, Conv, [256, 3, 2]],
   [[-1, 14], 1, Concat, [1]],  # cat head P4
   [-1, 3, C3, [512, False]],
   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1]],  # 22 (P4/16-medium)

   [-2, 1, Conv, [512, 3, 2]],
   [[-1, 10], 1, Concat, [1]],  # cat head P5
   [-1, 3, C3, [1024, False]],
   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1]],  # 26 (P5/32-large)

   [[], 1, Detect, [nc, anchors]],  # Detect(P5, P4, P3)
  ]